# Next Word Prediction using LSTM

This project demonstrates next word prediction model built using a Long Short-Term Memory (LSTM) Recurrent Neural Network (RNN) with TensorFlow and Keras. The model is trained on Shakespeare's Hamlet text to predict the next word given a sequence of preceding words.

## Project Description

The goal of this project is to train a neural network to predict the next word in a sequence of text. We use the Hamlet text from the NLTK Gutenberg corpus as our training data. The text is preprocessed by tokenizing words and creating input sequences. These sequences are then padded to a fixed length. An LSTM model is built, trained on this data, and then used to predict the next word for a given input phrase.

## Technologies Used

*   Python
*   TensorFlow
*   Keras
*   NLTK
*   Pandas
*   NumPy

## Setup and Installation

1.  **Clone the repository:**
Use code with caution
bash git clone git clone https://github.com/nvdpsingh/PredictingNextWordLSTM/

3.  **Install the required libraries:**
Use code with caution
bash pip install tensorflow pandas numpy nltk

4.  **Download the NLTK Gutenberg corpus:**
    Run the following code in your Python environment or Colab notebook:
Use code with caution
python import nltk nltk.download('gutenberg')

## How to Run the Code

The project can be run as a Python script or in a Jupyter Notebook/Google Colab environment.

1.  **Download the Hamlet text:** The code in the notebook automatically downloads and saves the 'shakespeare-hamlet.txt' from the Gutenberg corpus to a file named 'hamlet.txt'.
2.  **Run the notebook/script:** Execute the cells in the provided notebook or run the Python script. This will perform the following steps:
    *   Load and preprocess the text data.
    *   Create input sequences and labels.
    *   Build and compile the LSTM model.
    *   Train the model.
    *   Save the trained model (`next_word_lstm.h5`) and the tokenizer (`tokenizer.pickle`).
3.  **Predict the next word:** Use the `predict_next_word` function with your trained model and tokenizer to get predictions.

## Code Explanation

*   **Data Loading and Preprocessing:** The code loads the Hamlet text, converts it to lowercase, and uses `Tokenizer` to create word indices. Input sequences are generated by taking n-grams from the text.
*   **Padding:** `pad_sequences` is used to ensure all input sequences have the same length.
*   **Model Definition:** A `Sequential` Keras model is defined with an `Embedding` layer, `LSTM` layers, and a `Dense` output layer with a softmax activation for multi-class classification (predicting the next word).
*   **Training:** The model is compiled with `categorical_crossentropy` loss and the 'adam' optimizer and trained on the preprocessed data.
*   **Prediction Function:** The `predict_next_word` function takes the trained model, tokenizer, an input text sequence, and the maximum sequence length to predict the next word.

## Usage

After training the model, you can use the `predict_next_word` function like this:
Use code with caution
python

Load the trained model and tokenizer if not already in memory
from tensorflow.keras.models import load_model
import pickle
model = load_model("next_word_lstm.h5")
with open('tokenizer.pickle', 'rb') as handle:
tokens = pickle.load(handle)
input_text = "To be or not to be" max_sequence_len = model.input_shape[1] + 1 # Get max sequence length from model next_word = predict_next_word(model, tokens, input_text, max_sequence_len) print(f"Input text: {input_text}") print(f"Next Word Prediction: {next_word}")

input_text = " Barn. Last night of all,When yond same" next_word = predict_next_word(model, tokens, input_text, max_sequence_len) print(f"Input text: {input_text}") print(f"Next Word Prediction: {next_word}")

## Future Improvements

*   Experiment with different text datasets.
*   Tune the LSTM model architecture and hyperparameters.
*   Implement beam search or other decoding strategies for better predictions.
*   Add a user interface for easier interaction.

## License

This project is licensed under the terms of the GNU General Public License v3.0. See the [LICENSE](LICENSE) file for more details.
